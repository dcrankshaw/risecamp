{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"#API-Overview\" data-toc-modified-id=\"API-Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>API Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Some-Context-[Optional]\" data-toc-modified-id=\"Some-Context-[Optional]-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Some Context [Optional]</a></span></li><li><span><a href=\"#Create-a-ClipperConnection\" data-toc-modified-id=\"Create-a-ClipperConnection-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Create a ClipperConnection</a></span></li><li><span><a href=\"#Start-Clipper\" data-toc-modified-id=\"Start-Clipper-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Start Clipper</a></span></li><li><span><a href=\"#Deploy-a-model\" data-toc-modified-id=\"Deploy-a-model-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Deploy a model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-the-model\" data-toc-modified-id=\"Create-the-model-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Create the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution\" data-toc-modified-id=\"Solution-1.4.1.1\"><span class=\"toc-item-num\">1.4.1.1&nbsp;&nbsp;</span>Solution</a></span></li></ul></li><li><span><a href=\"#Deploy-to-Clipper\" data-toc-modified-id=\"Deploy-to-Clipper-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Deploy to Clipper</a></span></li><li><span><a href=\"#A-Note-About-Types-[Optional]\" data-toc-modified-id=\"A-Note-About-Types-[Optional]-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>A Note About Types [Optional]</a></span></li></ul></li><li><span><a href=\"#Register-an-application\" data-toc-modified-id=\"Register-an-application-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Register an application</a></span></li><li><span><a href=\"#Inspecting-Clipper\" data-toc-modified-id=\"Inspecting-Clipper-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Inspecting Clipper</a></span></li><li><span><a href=\"#Updating-the-Model\" data-toc-modified-id=\"Updating-the-Model-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Updating the Model</a></span></li><li><span><a href=\"#Adding-Model-Replicas\" data-toc-modified-id=\"Adding-Model-Replicas-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Adding Model Replicas</a></span></li></ul></li><li><span><a href=\"#Debugging-a-model\" data-toc-modified-id=\"Debugging-a-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Debugging a model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise-1\" data-toc-modified-id=\"Exercise-1-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Exercise 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Debugging-your-model\" data-toc-modified-id=\"Debugging-your-model-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Debugging your model</a></span></li><li><span><a href=\"#Exercise-one-solution\" data-toc-modified-id=\"Exercise-one-solution-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Exercise one solution</a></span></li><li><span><a href=\"#Exercise-Two\" data-toc-modified-id=\"Exercise-Two-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Exercise Two</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hint\" data-toc-modified-id=\"Hint-2.1.3.1\"><span class=\"toc-item-num\">2.1.3.1&nbsp;&nbsp;</span>Hint</a></span></li><li><span><a href=\"#Exercise-Two-Solution\" data-toc-modified-id=\"Exercise-Two-Solution-2.1.3.2\"><span class=\"toc-item-num\">2.1.3.2&nbsp;&nbsp;</span>Exercise Two Solution</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Example-Application---Birds-vs-Airplanes\" data-toc-modified-id=\"Example-Application---Birds-vs-Airplanes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Example Application - Birds vs Airplanes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Cifar\" data-toc-modified-id=\"Load-Cifar-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Load Cifar</a></span></li><li><span><a href=\"#Create-an-application\" data-toc-modified-id=\"Create-an-application-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create an application</a></span></li><li><span><a href=\"#Start-serving\" data-toc-modified-id=\"Start-serving-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Start serving</a></span></li><li><span><a href=\"#Train-Logistic-Regression-Model\" data-toc-modified-id=\"Train-Logistic-Regression-Model-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Train Logistic Regression Model</a></span></li><li><span><a href=\"#Deploy-Logistic-Regression-Model\" data-toc-modified-id=\"Deploy-Logistic-Regression-Model-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Deploy Logistic Regression Model</a></span></li><li><span><a href=\"#Load-TensorFlow-Model\" data-toc-modified-id=\"Load-TensorFlow-Model-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Load TensorFlow Model</a></span></li><li><span><a href=\"#Deploy-TensorFlow-Model\" data-toc-modified-id=\"Deploy-TensorFlow-Model-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Deploy TensorFlow Model</a></span></li></ul></li><li><span><a href=\"#Restarting-Clipper\" data-toc-modified-id=\"Restarting-Clipper-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Restarting Clipper</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import subprocess32 as subprocess\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Overview\n",
    "\n",
    "In the first part of this tutorial, you will explore how to create and interact with a Clipper cluster. The primary way of managing Clipper is with the Clipper Admin Python tool. This tutorial will walk you through all the things you can do with the Clipper Admin tool as well as explain what happens within Clipper when you issue each command.\n",
    "\n",
    "**Goal:** Be familiar with how to create and manage a Clipper cluster, and understand what happens when you issue Clipper admin commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Some Context [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Clipper Admin tool is distributed through with Pip. You can install it with `pip install clipper_admin`, but it has already been installed in this notebook for you.\n",
    "\n",
    "Clipper is built on top of Docker containers. A running Clipper cluster consists of a collection of Docker containers communicating with each over the network. As you issue commands against Clipper, you are communicating with these containers as well as creating new ones or destroying existing ones. As you explore the Clipper API throughout this exercise, we have illustrated how each command effects the cluster state. (*TODO(crankshaw): wording*).\n",
    "\n",
    "The main API for interacting with Clipper is exposed via a [`ClipperConnection`](http://docs.clipper.ai/en/develop/#clipper-connection) object. This is your handle to a Clipper cluster (this collection of Docker containers), and can be used to start, stop, inspect, and modify the cluster.\n",
    "\n",
    "In order to create a `ClipperConnection` object, you must provide it with a [`ContainerManager`](http://docs.clipper.ai/en/develop/#container-managers). While Docker is becoming an increasingly standard mechanism for deploying applications, there are many different tools for managing a Docker cluster. These tools broadly fall into the category of *Container Orchestration frameworks*. Some popular examples are [Kubernetes](https://kubernetes.io/), [Docker Swarm](https://docs.docker.com/engine/swarm/), and [DC/OS](https://dcos.io/). One of the reasons we run Clipper in Docker containers is to make the system as general as possible and support many different deployment scenarios. We abstract away all of the Docker container-specific commands behind the `ContainerManager` interface. The `ClipperConnection` object makes Clipper-specific decisions about how to issue commands, and then makes any changes to the Docker configuration (for example, to launch a container for a newly deployed model) through the `ContainerManager`. To support different container orchestration frameworks that manage Docker containers in different ways, we create different implementations of the `ContainerManager` interface.\n",
    "\n",
    "Clipper currently provides two `ContainerManager` implementations: the `DockerContainerManager` and the `KubernetesContainerManager`. In this exercise, you will be using the `DockerContainerManager`, which runs Clipper directly on your local Docker instance. This `ContainerManager` is particularly useful for trying out Clipper without needing to set up an enterprise-grade container orchestration framework. The `DockerContainerManager` is not recommended for production use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ClipperConnection\n",
    "\n",
    "First, create a new [`ClipperConnection`](http://docs.clipper.ai/en/develop/#clipper-connection) object with the type of `ContaineManager` you want to use. Simply creating a new connection object does not connect to Clipper. This is a good thing, because you haven't started Clipper yet so you would have nothing to connect to!\n",
    "\n",
    ">*The command to get the Docker IP address is a bit of hack to deal with the fact that you are running this exercise inside it's own Docker container. Without going into the details of how Docker handles container networking, what this command does is figure out how to send commands to \"localhost\" on the *host machine* from inside a Docker container. If you're curious, [this StackOverflow answer](https://stackoverflow.com/a/24326540/814642) provides a nice overview of the problem and the solution.*\n",
    "\n",
    "> *Generally, you will not have to deal with this and can leave the `docker_ip_address` argument as the default of `127.0.0.1` (localhost).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from clipper_admin import ClipperConnection, DockerContainerManager\n",
    "docker_ip = subprocess.check_output(\"./get_docker_ip.sh\").strip()\n",
    "clipper_conn = ClipperConnection(DockerContainerManager(docker_ip_address=docker_ip, redis_port=6380))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Clipper\n",
    "\n",
    "This command will start three Docker containers:\n",
    "\n",
    "1. The query frontend: This is the core Clipper container that you issue prediction requests against.\n",
    "2. The management frontend: This container manages and updates Clipper's internal configuration state.\n",
    "3. A Redis instance: Redis is used to persistently store Clipper's internal configuration state. You started Redis on port 6380 instead of the default port to avoid collisions with any Redis instances that are already running.\n",
    "\n",
    "> *Because Docker must download the Docker images from the internet (if they are not already cached) before it can start the containers, the first time you run this command it can take a long time to complete (up to a couple minutes). Thanks for your patience.*\n",
    "\n",
    "**TODO(crankshaw):** Illustration of Clipper Docker configuration\n",
    "\n",
    "If you try to start more than one Clipper cluster at once on the same host the second time you run the command it will fail because by default the second cluster will try to bind to the same ports as the first one. If you run into problems with the exercise and want to start over, see [Section 4](#Restarting-Clipper) for instructions on how to reset Clipper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.start_clipper()\n",
    "clipper_addr = clipper_conn.get_query_addr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now list the running Docker containers, filtering out any containers not started by Clipper, you should see the three Clipper containers listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!docker ps --filter label=ai.clipper.container.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a model\n",
    "\n",
    "\n",
    "At its most basic, a trained model is just a function that takes some input and produces some output. As a result, one way to think about Clipper is as a function server. While these functions are often comple models, Clipper is not restricted to serving machine learning models.\n",
    "\n",
    "To start with, you will deploy a very simple function to Clipper. You'll start with the classic \"Big Data\" version of Hello World: word count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "Before you can deploy a model to Clipper, you have to write it.\n",
    "\n",
    "Define a function that takes in a document (a string of text) and returns the number of words in the text as an integer.\n",
    "\n",
    "```py\n",
    "def count_words()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "solution": "hidden"
   },
   "outputs": [],
   "source": [
    "test_one = \"Hello world. Foo bar baz.\"\n",
    "assert count_words(test_one) == 5\n",
    "test_two = \"I'm learning so much at the inaugural RISE Camp.\"\n",
    "assert count_words(test_two) == 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "hidden"
   },
   "source": [
    "Many machine learning models can take advantage of data parallelism when performing inference to improve performance. Because of this, Clipper may provide multiple inputs at once to a deployed multiple and so models deployed to Clipper must have a function interface that takes a list of inputs as an argument and returns a list of predictions as strings. Returning predictions as strings provides a lot of flexibility over what your models can return. Commonly, model in Clipper will return either a single number (a label or score) or JSON containing a richer representation of the model ouput (for example, by including confidence estimates).\n",
    "\n",
    "To adapt your word count example to match the required interface, we define a second function that takes a list of documents as an argument and returns the word count of each document as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function you deploy to Clipper must take a list of inputs, so we\n",
    "# wrap our core word-count logic in loop.\n",
    "def count_words_in_docs(docs):\n",
    "    counts = []\n",
    "    for d in docs:\n",
    "        count = str(count_words(d))\n",
    "        counts.append(count)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert count_words_in_docs([test_one, test_two]) == [\"5\", \"9\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```py\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to Clipper\n",
    "\n",
    "Clipper provides a collection of model deployer modules to simplify the process of deploying a trained model to Clipper and avoid the need to figure out how to save models and build custom Docker containers capable of serving the saved models for some common use cases. With these modules, you can deploy models directly from Python to Clipper.\n",
    "\n",
    "Currently, Clipper provides two deployer modules, one to deploy arbitrary Python functions (within some constraints) and the other to deploy PySpark models along with pre- and post-processing logic. In this exercise, you will be using the Python deployer.\n",
    "\n",
    "[Read more about model deployers in the Clipper documentation.](http://docs.clipper.ai/en/latest/#model-deployers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from clipper_admin.deployers import python as python_deployer\n",
    "python_deployer.deploy_python_closure(\n",
    "    clipper_conn,\n",
    "    name=\"wordcount\",  # The name of the model in Clipper\n",
    "    version=1,  # A unique identifier to assign to this model.\n",
    "    input_type=\"string\",  # The type of data the model function expects as input\n",
    "    func=count_words_in_docs # The model function to deploy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clipper deploys each model in its own Docker container. After deploying the model, Clipper used the `DockerContainerManager` to start a container for this model and create an RPC connection with the Clipper query frontend, as illustrated below.\n",
    "\n",
    "> *Once again, Clipper must download a Docker container from the internet the first time this command is run.*\n",
    "\n",
    "**TODO(crankshaw):** Show new Docker container\n",
    "\n",
    "If you list the Clipper containers again, you can see the container running your word count model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!docker ps --filter label=ai.clipper.container.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### A Note About Types [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When you deploy models and register applications, you must specify the input type that the model or application expects. The type that you specify has implications for how Clipper manages input serialization and deserialization. From the user's perspective, the input type affects the behavior of Clipper in two places. In the \"input\" field of the request JSON body, applications will reject requests where the value of that field is the wrong type. And the deployed model function will be called with a list of inputs of the specified type.\n",
    "\n",
    "The input type can be one of the following types:\n",
    "+ *\"ints\"*: The value of the \"input\" field in a request must be a JSON list of ints. The model function will be called with a list of numpy arrays of type `numpy.int`.\n",
    "+ *\"floats\"*: The value of the \"input\" field in a request must be a JSON list of doubles. The model function will be called with a list of numpy arrays of type `numpy.float32`.\n",
    "+ *\"doubles\"*: The value of the \"input\" field in a request must be a JSON list of doubles. The model function will be called with a list of numpy arrays of type `numpy.float64`.\n",
    "+ *\"bytes\"*: The value of the \"input\" field in a request must be a Base64 encoded string. The model function will be called with a list of numpy arrays of type `numpy.int8`.\n",
    "+ *\"strings\"*: The value of the \"input\" field in a request must be a string. The model function will be called with a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register an application\n",
    "\n",
    "You've now deployed a model to Clipper, but you don't have any way to query it yet. Instead of automatically creating a REST endpoint when you deploy a model, Clipper introduces a layer of indirection: the application. Clients query a specific application in Clipper, and the application routes the query to the correct model. This allows multiple applications to route queries to the same model, and in the future will allow a single application to route queries to multiple models. A single Clipper cluster can have many applications registered and many models deployed at once.\n",
    "\n",
    "When you register an application you must specify certain parts of the applications behavior. These include:\n",
    "+ The name to give the REST endpoint.\n",
    "+ The input type that the application expects (Clipper will ensure applications only route requests to models with matching input types).\n",
    "+ The latency service level objective (SLO) specified in microseconds. Clipper will manager how it schedules and routes queries for an application based on the specified service level objective. For example, Clipper will set the amount of time it allows requests to spend queued before being sent to the model based on the service level objective for the application. In addition, Clipper will respond to requests by the end of the specified SLO, even if it has not received a prediction back from the model.\n",
    "+ The default output: Clipper will respond with the default output to requests if a real prediction isn't available by the end of the service level objective. In Part 2 of this tutorial, you will spend some time debugging applications that keep responding with the default application and learn about some common reasons that models fail to respond in time.\n",
    "\n",
    "\n",
    "When you register an application with Clipper, it creates a REST endpoint for that application:\n",
    "\n",
    "```\n",
    "URL: /<app_name>/predict\n",
    "Method: POST\n",
    "Data Params: {\"input\": <input>}\n",
    "```\n",
    "\n",
    "Register an application to query your word count function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.register_application(\n",
    "    name=\"wordcount-app\",\n",
    "    input_type=\"strings\",\n",
    "    default_output=\"-1\",\n",
    "    slo_micros=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try querying the newly created application with [curl](https://curl.haxx.se/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$clipper_addr\"\n",
    "curl -s -X POST --header \"Content-Type:application/json\" \\\n",
    "    -d '{\"input\": \"The sky above the port was the color of television, tuned to a dead channel.\"}' \\\n",
    "    $1/wordcount-app/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that your application returned the default output of \"-1\". This is because even though you have deployed a model and registered an application, you have not told Clipper to route requests from the \"wordcount-app\" application to the \"wordcount\" model.\n",
    "\n",
    "You do this by *linking* the model to the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.link_model_to_app(app_name=\"wordcount-app\", model_name=\"wordcount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you query the \"wordcount-app\" endpoint again, Clipper should return the word count. Try it with your own phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$clipper_addr\"\n",
    "curl -X POST --header \"Content-Type:application/json\" \\\n",
    "    -d '{\"input\": \"TRY YOUR OWN PHRASE\"}' \\\n",
    "    $1/wordcount-app/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Clipper\n",
    "\n",
    "The `ClipperConnection` object has several methods to inspect various aspects of the Clipper cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can list all of the applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.get_all_apps(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or all of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.get_all_models(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clipper also tracks several performance metrics that you can inspect at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.inspect_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also fetch the raw container logs from all of the Clipper docker containers. The command will print the paths to the log files for further examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.get_clipper_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be examing the metrics from `inspect_instance()` and the container logs to debug models and applications in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models are rarely static. Instead, data science tends to be an iterative process, with new and improved models being developed over time. Clipper supports this workflow by letting you deploy new versions of models. If you look back to where you linked your wordcount model to the application, you'll see that there is no mention of versioning in that method call. Instead, when a new version of a model is deployed, Clipper will automatically start routing requests to the new version.\n",
    "\n",
    "Create a new version of the \"wordcount\" model that counts the number of *unique* words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "def unique_word_count(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    text = text.translate(None, string.punctuation)\n",
    "    words = text.split()\n",
    "    counts = {}\n",
    "    for w in words:\n",
    "        if w in counts:\n",
    "            counts[w] += 1\n",
    "        else:\n",
    "            counts[w] = 1\n",
    "    return len(counts.keys())\n",
    "    \n",
    "def count_unique_words_multiple_docs(docs):\n",
    "    counts = []\n",
    "    for d in docs:\n",
    "        count = unique_word_count(d)\n",
    "        counts.append(str(count))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq_example = \"Hello world. This is an example sentence. This is another sentence. There are some repeated words in this document.\"\n",
    "count_unique_words_multiple_docs([word_freq_example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy this new version of the function as version \"2\". For this application, you are using a numeric versioning scheme. But Clipper just treats versions as unique string identifiers, so you could use other versioning schemes (such as Git hashes or semantic versioning). Versions don't even have to be ordered, Clipper just tracks the currently active version.\n",
    "\n",
    "**TODO(crankshaw):** Show new Docker container version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_deployer.deploy_python_closure(\n",
    "    clipper_conn,\n",
    "    name=\"wordcount\",\n",
    "    version=\"2\",\n",
    "    input_type=\"string\",  # The input type must match across all model versions\n",
    "    func=count_unique_words_multiple_docs # The new function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$clipper_addr\"\n",
    "curl -X POST --header \"Content-Type:application/json\" \\\n",
    "    -d '{\"input\": \"The sky above the port was the color of television, tuned to a dead channel.\"}' \\\n",
    "    $1/wordcount-app/predict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the \"new and improved\" model is not actually improved. If you deploy a model that isn't working well, you can roll back to any previous version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.set_model_version(name=\"wordcount\", version=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$clipper_addr\"\n",
    "curl -X POST --header \"Content-Type:application/json\" \\\n",
    "    -d '{\"input\": \"The sky above the port was the color of television, tuned to a dead channel.\"}' \\\n",
    "    $1/wordcount-app/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Model Replicas\n",
    "\n",
    "Many machine learning models are computationally expensive and a single instance of the model may not meet the throughput demands of a serving workload. To increase prediction throughput, you can add additional replicas of a model. This creates additional Docker containers running the same model. Clipper will act as a load-balancer and distribute incoming requests across the set of available model replicas to provide higher throughputs.\n",
    "\n",
    "Set the number of replicas for the currently active version (\"1) of the\"wordcount\" model to 3.\n",
    "\n",
    "**TODO(crankshaw): illustrate additional containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.set_num_replicas(\"wordcount\", num_replicas=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you list the Clipper Docker containers, you should now see four containers based on the image \"wordcount:1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!docker ps --filter label=ai.clipper.container.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to reduce the number of replicas of a model to free up hardware resource, you can use the same command.\n",
    "\n",
    "Set the number of replicas for \"wordcount\" back to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.set_num_replicas(\"wordcount\", num_replicas=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!docker ps --filter label=ai.clipper.container.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging a model\n",
    "\n",
    "Making mistakes and accidentally writing buggy code is an inevitable part of software development. In part 2 of this exercise, you will deploy some buggy models to Clipper and learn how to identify and fix the bugs.\n",
    "\n",
    "**Goal:** Learn how to troubleshoot Clipper and understand some common sources of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In this exercise, you will deploy a model that computes a histogram of the relative frequency of words in a document. It is a slight modification of the way we counted unique words in the previous exercise.\n",
    "\n",
    "Initially, this function will have some problems which you will find and fix.\n",
    "\n",
    "Start by creating an application and deploying the original version of the model to Clipper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_freq_1(docs):\n",
    "    doc_frequencies = []\n",
    "    for d in docs:\n",
    "        # Convert to lower case\n",
    "        text = text.lower()\n",
    "        text = text.translate(None, string.punctuation)\n",
    "        words = text.split()\n",
    "        counts = {}\n",
    "        for w in words:\n",
    "            if w in counts:\n",
    "                counts[w] += 1\n",
    "            else:\n",
    "                counts[w] = 1\n",
    "        doc_frequencies.append(counts)\n",
    "    return doc_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_deployer.deploy_python_closure(\n",
    "    clipper_conn,\n",
    "    name=\"wordfreq\",\n",
    "    version=\"1\",\n",
    "    input_type=\"string\",  # The input type must match across all model versions\n",
    "    func=word_freq_1 # The new function\n",
    ")\n",
    "\n",
    "clipper_conn.register_application(name=\"exercise-one\", input_type=\"strings\", slo_micros=100000, default_output=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now try to query the model and see what happens (this time we are going to use the excellent [Python Requests](http://docs.python-requests.org/en/master/) library to query Clipper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "def query(doc):\n",
    "    url = \"http://{}/exercise-one/predict\".format(clipper_addr)\n",
    "    req_json = json.dumps({'input': doc})\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    start = datetime.now()\n",
    "    r = requests.post(url, headers=headers, data=req_json)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query(\"Peter Piper picked a peck of pickled peppers. How many pickled peppers did Peter Piper pick?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query(\"How much wood could a wood chuck chuck if a wood chuck could chuck wood?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the application will always return the default prediction. This means that your deployed word frequency model is not returning predictions in time for some reason. Uh oh!\n",
    "\n",
    "Time to start debugging..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging your model\n",
    "\n",
    "A good place to start is to make sure your application and model are registered and linked. Use the commands from part one and to inspect Clipper and make sure you've deployed your model and application correctly. The \"default_explanation\" field in the query response should give you a hint to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this cell to get info about Clipper's current configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once you've found and fixed any problems with your application configuration, try querying your application again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query(\"Peter Piper picked a peck of pickled peppers. How many pickled peppers did Peter Piper pick?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should still be seeing only default predictions, but the explanation for the default response should have changed. Now that you've verified that your application is correctly configured with Clipper, it's time to figure out why your model is not returning responses in time.\n",
    "\n",
    "The best place to start is to look at the logs from your model container. Use the `get_clipper_logs()` function to fetch the logs. You can use the Jupyter notebook file navigator to examine the logs in a new tab, or print them directly in the cell using the shell command `cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat clipper_logs/logfile.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logs for the model container should help you identify the problem with the `word_freq_1` model. Once you've identified and fixed it, deploy a the corrected function as version 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_freq_correct(docs):\n",
    "    # Fix the function here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "python_deployer.deploy_python_closure(\n",
    "    clipper_conn,\n",
    "    name=\"wordfreq\",\n",
    "    version=\"2\",\n",
    "    input_type=\"string\",\n",
    "    func=word_freq_correct\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you query the function a final time, you should see the `default` field of the response is false and the output contains the correct word frequencies.\n",
    "\n",
    "**TODO(crankshaw):** Add example output showing the correct frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Exercise One Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** The first problem was that your application was not linked to your model. Run the following command to link them.\n",
    "\n",
    "```py\n",
    "clipper_conn.link_model_to_app(app_name=\"exercise-one\", model_name=\"wordfreq\")\n",
    "```\n",
    "\n",
    "**2.** The second problem was that your function was not returning a list of strings, but instead a list of Python dict objects. The solution is to convert the dicts to strings before you return them. A good way to serialize Python dicts into strings is using JSON. The following function should work.\n",
    "\n",
    "```py\n",
    "def word_freq_correct(docs):\n",
    "    doc_frequencies = []\n",
    "    for d in docs:\n",
    "        # Convert to lower case\n",
    "        text = text.lower()\n",
    "        text = text.translate(None, string.punctuation)\n",
    "        words = text.split()\n",
    "        counts = {}\n",
    "        for w in words:\n",
    "            if w in counts:\n",
    "                counts[w] += 1\n",
    "            else:\n",
    "                counts[w] = 1\n",
    "        doc_frequencies.append(json.dumps(counts))\n",
    "    return doc_frequencies\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise Two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will debug a new model using the same process as before. This model will not work for a different, but the same debugging process should help isolate the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(crankshaw): Can we come up with something more interesting???\n",
    "import numpy as np\n",
    "def random_project(xs):\n",
    "    time.sleep(1)\n",
    "    outputs = []\n",
    "    for x in xs:\n",
    "        output = np.dot(x, np.random.normal(size=len(x)))\n",
    "    outputs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the exercise:\n",
    "python_deployer.deploy_python_closure(\n",
    "    clipper_conn,\n",
    "    name=\"random-project\",\n",
    "    version=\"1\",\n",
    "    input_type=\"doubles\",\n",
    "    func=random_project\n",
    ")\n",
    "\n",
    "clipper_conn.register_application(\n",
    "    name=\"exercise-two\",\n",
    "    input_type=\"doubles\",\n",
    "    slo_micros=50000,\n",
    "    default_output=\"DEFAULT\")\n",
    "\n",
    "clipper_conn.link_model_to_app(app_name=\"exercise-two\", model_name=\"random-project\")\n",
    "\n",
    "def query(x):\n",
    "    url = \"http://{}/exercise-two/predict\".format(clipper_addr)\n",
    "    req_json = json.dumps({'input': x})\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    start = datetime.now()\n",
    "    r = requests.post(url, headers=headers, data=req_json)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to modify the original `random_project` model to fix it and deploy the new version to Clipper before the assert statement will pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This assert will fail until the application stops returning default predictions.\n",
    "\n",
    "assert not query(np.random.normal(size=100)).default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Hint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Look at the timing of how long each prediction takes in the model container. You can get this from the Clipper metrics (`inspect_instance()`) or from the model container logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Exercise Two Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the model is taking too long to make a prediction and so the application requests are hitting the deadline set by the latency SLO before the model can return a prediction. The solution is to remove the call to `time.sleep()` from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Application - Birds vs Airplanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 3 of this exercise, you will build a real machine learning application that uses computer vision models to classify images, including training your own models.\n",
    "\n",
    "You will create an application that labels images from the CIFAR-10 dataset as containing either birds or planes.\n",
    "\n",
    "These images have already been downloaded and are available locally at `~/cifar`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cifar\n",
    "\n",
    "The first step in building any application, using machine-learning or otherwise, is to understand the application requirements. Load the dataset into the notebook so you can examine it and better understand the dataset you will be working with. The `cifar_utils` library provides several utilities for working with CIFAR data – we will make use of one of them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cifar_utils\n",
    "\n",
    "cifar_loc = os.path.expanduser(\"cifar/\")\n",
    "test_x, test_y = cifar_utils.filter_data(\n",
    "    *cifar_utils.load_cifar(cifar_loc, cifar_filename=\"cifar_test.data\", norm=True))\n",
    "train_x, train_y = cifar_utils.filter_data(\n",
    "    *cifar_utils.load_cifar(cifar_loc, cifar_filename=\"cifar_train.data\", norm=True))\n",
    "raw_x, raw_y = cifar_utils.filter_data(\n",
    "    *cifar_utils.load_cifar(cifar_loc, cifar_filename=\"cifar_test.data\", norm=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data you've loaded. The size and blurriness of these photos should give you a better understanding of the difficulty of the task you will ask of your machine learning models! If you'd like to see more images, increase the number of rows of images displayed -- the last argument to the function -- to a number greater than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "cifar_utils.show_example_images(raw_x, raw_y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an application\n",
    "\n",
    "For this tutorial, create an application named \"cifar-binary-classifier\". Note that Clipper allows you to create the application before deploying any models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_name = \"cifar-binary-classifier\"\n",
    "# If the model (which we will later link to our application) doesn't\n",
    "# return a prediction in time, predict label 0 (bird) by default\n",
    "default_output = \"0\"\n",
    "\n",
    "clipper_conn.register_application(\n",
    "    name=app_name,\n",
    "    input_type=\"doubles\",\n",
    "    default_output=default_output,\n",
    "    slo_micros=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you list the applications registered with Clipper, you should see the newly registered \"cifar-binary-classifier\" application show up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.get_all_apps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start serving\n",
    "\n",
    "Now that you have registered an application, you can start querying the application for predictions. \n",
    "\n",
    "You will now start querying Clipper with a simple Python frontend app that computes the average accuracy of the responses after every 100 requests and updates a plot of the results with every iteration.\n",
    "\n",
    "**[Go to the query_cifar notebook to start the app.](query_cifar.ipynb) Make sure to leave the query_cifar notebook open and the cell running as you complete the rest of this exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression Model\n",
    "\n",
    "When tackling a new problem with machine learning, it's always good to start with simple models and only add complexity when needed. Start by training a logistic regression binary classifier using [Scikit-Learn](http://scikit-learn.org/). This model gets about 68% accuracy on the offline evaluation dataset if you use 10,000 training examples. It gets about 74% if you use all 50,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm \n",
    "def train_sklearn_model(m, train_x, train_y):\n",
    "    m.fit(train_x, train_y)\n",
    "    return m\n",
    "lr_model = train_sklearn_model(lm.LogisticRegression(), train_x, train_y)\n",
    "print(\"Logistic Regression test score: %f\" % lr_model.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Logistic Regression Model\n",
    "\n",
    "While 68-74% accuracy on a CIFAR binary classification task is significantly below state of the art, it's already much better than the 50% accuracy your application yields right now by guessing randomly.\n",
    "\n",
    "To deploy your Scikit-Learn logistic regression model, you can use the Python model deployer module you've been using throughout the tutorial. In particular, because the Scikit-Learn model can be pickled, you can wrap it in a function closure and deploy that function directly as a model to Clipper without needing to manually save the model or write a Docker\n",
    "container.\n",
    "\n",
    "First, you will write and test the prediction function to deploy. This function must take a list of inputs (a list of CIFAR images in this example) as the only function argument and return a list of predictions as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sklearn_predict(images):\n",
    "    preds = lr_model.predict(images)\n",
    "    return [str(p) for p in preds]\n",
    "\n",
    "print(\"Predicted labels: {}\".format(sklearn_predict(test_x[0:3])))\n",
    "print(\"Correct labels: {}\".format(test_y[0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have defined and tested your prediction function, deploy it to Clipper as a model named \"cifar-model\". This time, specify the \"input_type\" field as \"doubles.\" Don't forget to link the model to the application when after you've deployed it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"cifar-model\"\n",
    "\n",
    "# Insert the call to deploy the model here:\n",
    "raise NotImplementedError\n",
    "# And link the model to the application:\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've deployed and linked your model to your app, go ahead and check back on your running frontend application in the [query_cifar notebook](query_cifar.ipynb). You should see the accuracy rise from around 50% to the accuracy of your SKLearn model (68-74%), without having to stop or modify your application at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TensorFlow Model\n",
    "\n",
    "To improve the accuracy of your application further, you will now deploy a TensorFlow convolutional neural network. This model takes a few hours to train, so you will download the trained model parameters rather than training it from scratch. This model gets about 88% accuracy on the test dataset.\n",
    "\n",
    "We have provided a pre-trained TensorFlow model stored at `tf_cifar_model/cifar10_model_full`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf_cifar_model_path = os.path.abspath(\"tf_cifar_model/cifar10_model_full\")\n",
    "# Load the saved model\n",
    "tf_session = tf.Session('', tf.Graph())\n",
    "with tf_session.graph.as_default():\n",
    "    saver = tf.train.import_meta_graph(\"%s.meta\" % tf_cifar_model_path)\n",
    "    saver.restore(tf_session, tf_cifar_model_path)\n",
    "\n",
    "# Score it on the evaluation dataset\n",
    "def tensorflow_score(session, test_x, test_y):\n",
    "    \"\"\"\n",
    "    NOTE: This predict method expects pre-whitened (normalized) images\n",
    "    \"\"\"\n",
    "    logits = session.run('softmax_logits:0',\n",
    "                           feed_dict={'x:0': test_x})\n",
    "    relevant_activations = logits[:, [cifar_utils.negative_class, cifar_utils.positive_class]]\n",
    "    preds = np.argmax(relevant_activations, axis=1)\n",
    "    return float(np.sum(preds == test_y)) / float(len(test_y))\n",
    "print(\"TensorFlow ConvNet test score: %f\" % tensorflow_score(tf_session, test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy TensorFlow Model\n",
    "\n",
    "Unlike the Scikit-Learn model, TensorFlow models cannot be pickled. Instead, they must be saved using TensorFlow's native serialization API. Because of this, you cannot use the generic Python model deployer to deploy the model to Clipper. Instead, you must save the model yourself and specify a Docker container that knows how to load and run a TensorFlow model. We have provided a Docker image for you, `clipper/tf_cifar_container:develop`, that can run the CIFAR TensorFlow model. The Docker container will load and reconstruct the model from the serialized model checkpoint when the container is started.\n",
    "\n",
    "After completing this step and deploying the new model, Clipper will send queries to the newly-deployed TensorFlow model instead of the logistic regression Scikit-Learn model, improving the application's accuracy.\n",
    "\n",
    "> *Once again, Clipper must download this Docker image from the internet, so this may take a minute. Thanks for your patience.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.build_and_deploy_model(\n",
    "    name=model_name,\n",
    "    version=\"2\",\n",
    "    input_type=\"doubles\",\n",
    "    model_data_path=os.path.abspath(\"tf_cifar_model\"),\n",
    "    base_image=\"clipper/tf_cifar_container:develop\",\n",
    "    num_replicas=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the frontend again and see if the accuracy has improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restarting Clipper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into issues and want to completely stop Clipper, you can do this by calling [`ClipperConnection.stop_all()`](http://docs.clipper.ai/en/latest/#clipper_admin.ClipperConnection.stop_all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_conn.stop_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you list all the Docker containers a final time, you should see that all of the Clipper containers have been stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!docker ps --filter label=ai.clipper.container.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now call `clipper_conn.start_clipper()` again without running into errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 for Clipper",
   "language": "python",
   "name": "clipper_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
